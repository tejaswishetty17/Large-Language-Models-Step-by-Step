{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNCe1EqtZViHp8ArtkeaeuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaswishetty17/Large-Language-Models-Step-by-Step/blob/main/1_Introduction_to_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu2MYo7w5hqk"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load our model onto the GPU for faster inference. We will be using small model - microsoft/Phi-3-mini-4k-instruct.\n",
        "\n",
        "Lets see how we load the model and tokenizer separately."
      ],
      "metadata": {
        "id": "kk885n__6H59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "#Load model and Tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map = \"cuda\",\n",
        "    torch_dtype = \"auto\",\n",
        "    trust_remote_code=False,\n",
        "\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "p1Od2uRe58vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can now use the model and tokenizer directly using a much easier way, i.e. to wrap it in pipeline object"
      ],
      "metadata": {
        "id": "aZP_LvW-7pSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text = False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y845Siy7ZB6",
        "outputId": "03c63b3b-45c1-42e0-f122-81b4ea9fa2a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create the prompt as a user and pass it to the model"
      ],
      "metadata": {
        "id": "kIOcOe9684i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\":\"user\", \"content\":\"what is Language Models?\"}\n",
        "]\n",
        "\n",
        "output = generator(message)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRgmNUt482In",
        "outputId": "a4665f38-af00-45c5-8b80-091c27ef0ab2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Language models are a type of statistical or machine learning model that are used to predict the probability distribution of words in a given language. They are trained on large amounts of text data and learn the patterns and structures of a language, allowing them to generate or understand human language.\n",
            "\n",
            "Language models can be used for various natural language processing (NLP) tasks, such as text generation, machine translation, speech recognition, and sentiment analysis. Some popular language models include GPT-3 (Generative Pre-trained Transformer 3) by Microsoft, BERT (Bidirectional Encoder Representations from Transformers) by Google, and XLNet by Google.\n",
            "\n",
            "These models are built using deep learning techniques, specifically transformer architectures, which enable them to capture the context and relationships between words in a sentence. By understanding the context and structure of a language, language models can generate coherent and contextually relevant text, making them valuable tools for a wide range of applications in the field of artificial intelligence and NLP.\n"
          ]
        }
      ]
    }
  ]
}