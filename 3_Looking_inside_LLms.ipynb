{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOv+RU5AKNM6Mmtg70N0nOX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaswishetty17/Large-Language-Models-Step-by-Step/blob/main/3_Looking_inside_LLms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers>=4.41.2 accelerate>=0.31.0"
      ],
      "metadata": {
        "id": "-28EMzDuiJl4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Using a gated model from huggingface\n",
        "\n"
      ],
      "metadata": {
        "id": "i_zYHcLO5mWh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzfwkWXfhcuv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import os\n",
        "\n",
        "access_token = \"\"\n",
        "\n",
        "#Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", token = access_token)\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        "    token = access_token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a pipeline\n",
        "generation_gated = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer = tokenizer,\n",
        "    max_new_tokens=1000\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90QLxnPlCV4c",
        "outputId": "d0866873-f5ee-4c6e-ac2f-d10f9a2f99e9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Lets see the Inputs and Outputs of a Trained Transformer LLM\n",
        "\n"
      ],
      "metadata": {
        "id": "3N-9eGnT9ny1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "    {\"role\":\"user\", \"content\":\"Write a mail to send my resume to a HR who is recruiting for Data Scientist role. Also, add my github project details.\"}\n",
        "  ]\n",
        "output = generation_gated(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQjiXELV9nNs",
        "outputId": "48430bc5-fc80-429b-c28e-930c540cef0e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0]['generated_text'][1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q52QdXBy9lsE",
        "outputId": "14a6c3c9-f614-4b0d-8b43-9845216ac5a9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dear [HR Name],\n",
            "\n",
            "I am writing to express my interest in the Data Scientist role that you are currently recruiting for. I am excited about the opportunity to join your team and contribute my skills and experience to help drive your business forward.\n",
            "\n",
            "I have attached my resume, which provides a detailed overview of my education, work experience, and skills. I have a strong background in data analysis, machine learning, and statistical modeling, and I am confident that I have the skills and experience necessary to excel in this role.\n",
            "\n",
            "In addition to my resume, I would like to share with you some of my recent projects that I have worked on. I have a GitHub account where you can view my work, including a project on predicting customer churn for a telecom company and a project on analyzing customer sentiment for an e-commerce company. You can find my GitHub account at [insert link].\n",
            "\n",
            "Thank you for considering my application. I look forward to the opportunity to discuss my qualifications further.\n",
            "\n",
            "Sincerely,\n",
            "[Your Name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySI4_SCzCmkR",
        "outputId": "dff24ed1-848e-4559-b931-0b3c54ad6c0b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Lets see how the single token from a probability distribution is choosen(sampling/decoding)\n",
        "\n"
      ],
      "metadata": {
        "id": "fdDZ39ybFOm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The capital of Japan is\"\n",
        "\n",
        "#Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "#Get the output of the model before the lm_head\n",
        "model_output = model.model(input_ids)\n",
        "print(\"model_output: \\n\", model_output)\n",
        "\n",
        "#Get the output from the lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])\n",
        "print(\"\\n\\n lm_head_output: \\n\", lm_head_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o45zAFy9FX-m",
        "outputId": "8c5eadff-1f6b-4bd0-cb3a-4c42b22e6145"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_output: \n",
            " BaseModelOutputWithPast(last_hidden_state=tensor([[[-2.0469,  1.9531, -2.1250,  ..., -0.1250, -2.2656,  2.9531],\n",
            "         [ 0.6953,  3.4531,  1.4688,  ..., -4.3125,  1.0234,  0.4258],\n",
            "         [-7.0625, -1.6641,  4.0938,  ..., -4.3750, -2.0625, -2.6250],\n",
            "         [-2.2969,  2.5469, -0.1289,  ..., -2.0156,  2.3125, -2.5938],\n",
            "         [-2.1562,  2.5625, -1.4766,  ..., -1.0312,  5.5625, -2.4375],\n",
            "         [-0.7734,  2.3594,  1.0625,  ...,  2.3438,  9.6875, -2.1719]]],\n",
            "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>), past_key_values=DynamicCache(), hidden_states=None, attentions=None)\n",
            "\n",
            "\n",
            " lm_head_output: \n",
            " tensor([[[-4.6562, -4.4375, -0.0747,  ..., -3.4688, -2.5156, -3.0625],\n",
            "         [-7.1562, -7.0625,  0.4766,  ..., -6.0312, -4.7812, -4.1250],\n",
            "         [-6.8438, -7.6250,  0.3867,  ..., -6.0312, -6.6250, -6.0312],\n",
            "         [-6.4375, -6.7812, -0.2773,  ..., -5.0938, -6.0938, -4.8750],\n",
            "         [-7.8438, -8.4375,  3.7031,  ..., -9.4375, -8.5000, -6.5000],\n",
            "         [-7.0625, -7.3438,  1.5781,  ..., -7.8438, -7.6875, -5.0625]]],\n",
            "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = model.generate(input_ids)\n",
        "tokenizer.batch_decode(token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v52q67KoFMWF",
        "outputId": "6fbabf4b-7c37-41dc-98c1-1e83ab8841f3"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s> The capital of Japan is Tokyo. It is a bustling metropolis with a population of over 13 million people']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = lm_head_output[0,-1].argmax(-1)\n",
        "tokenizer.decode(token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RFFi7LGpIQOb",
        "outputId": "575e28f8-ee3d-42fe-87a6-91c9a606df8a"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tokyo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_output[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cNCiCp6In3P",
        "outputId": "56843549-6d5f-4240-c60d-43df88d357b8"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 4096])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot6BHofUIuW-",
        "outputId": "6df86e8d-2ca0-43c5-a517-c29277fd4f88"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 32000])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a mail to send my resume to a HR who is recruiting for Data Scientist role. Also, add my github project details.\"\n",
        "\n",
        "#Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "input_ids = input_ids.to(\"cuda\")"
      ],
      "metadata": {
        "id": "R4v16_qHI1wW"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1\n",
        "#Generate the text\n",
        "generation_output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    use_cache=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gPyJe5TJpBW",
        "outputId": "8d961cf9-886f-49cb-d1cf-9779c6c3c055"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.06 s ± 268 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1\n",
        "#Generate the text\n",
        "generation_output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    use_cache=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk67_VCDJ--a",
        "outputId": "8e439773-f848-442a-b450-843a2044b9ff"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.56 s ± 176 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    }
  ]
}